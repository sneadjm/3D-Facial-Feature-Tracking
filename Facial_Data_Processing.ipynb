{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2q27gKz1H20"
      },
      "source": [
        "##### Copyright 2023 The MediaPipe Authors. All Rights Reserved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "TUfAcER1oUS6"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_cQX8dWu4Dv"
      },
      "source": [
        "This notebook is based on a tutorial produced by The MediaPipe Authors, and contains the above licenses. The paper \"3D Facial Feature Tracking with\n",
        "Multimodal Depth Fusion\" uses the Face Landmarker and Face Detector model structures, weights, and loading procedures from MediaPipe to facilitate the facial feature tracking process. </br>\n",
        "\n",
        "The use of <b>laplace_max_step</b> function and the compilation of the results into cumulative .CSV files represent a divergence from the original notebook produced by MediaPipe for the application detailed in \"3D Facial Feature Tracking with\n",
        "Multimodal Depth Fusion\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a49D7h4TVmru"
      },
      "source": [
        "Install MediaPipe library and download the Face Landmarker v2 and Face Detector.\n",
        "\n",
        "\n",
        "Face Landmarker documentation can be found [here](https://developers.google.com/mediapipe/solutions/vision/face_landmarker#models).\n",
        "\n",
        "\n",
        "We will use the models \"as-is\" and will not do any further fine-tuning due to the relative adherence of our application with typical facial data training sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMjuVQiDYJKF"
      },
      "outputs": [],
      "source": [
        "!pip install -q mediapipe\n",
        "!wget -O face_landmarker_v2_with_blendshapes.task -q https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task\n",
        "!wget -q -O detector.tflite -q https://storage.googleapis.com/mediapipe-models/face_detector/blaze_face_short_range/float16/1/blaze_face_short_range.tflite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Es0Mnb5s_G0"
      },
      "source": [
        "Mount files stored in Google Drive for use by the code. Skip if running locally or in a different compute environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvMRSve2qGmN"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qcvFyQ4tEGJ"
      },
      "source": [
        "Defines the function that calculates where the focus is at a maximum for the purposes of 3D absolute depth estimation. Not needed if only doing x and y analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hez8TH19qnvn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "def laplace_max_step(pathname):\n",
        "  laplacemax = 0\n",
        "  for StepNo in os.scandir(pathname):\n",
        "    image = cv2.imread(pathname + str(os.path.basename(StepNo)), cv2.IMREAD_GRAYSCALE)\n",
        "    #makes sure that program doesn't crash if an image doesn't load right\n",
        "    if image is None:\n",
        "      stepmax = 471\n",
        "      continue\n",
        "    (x_max,y_max) = image.shape\n",
        "    image_face = mp.Image.create_from_file(pathname + str(os.path.basename(StepNo)))\n",
        "    face_result = face_detector.detect(image_face)\n",
        "\n",
        "    for detection in face_result.detections:\n",
        "      # Draw bounding_box\n",
        "      bbox = detection.bounding_box\n",
        "      #cv2.rectangle(annotated_image, start_point, end_point, TEXT_COLOR, 3)\n",
        "      #image = cv2.imread(pathname_max, cv2.IMREAD_GRAYSCALE)\n",
        "      img = Image.fromarray(image, 'L')\n",
        "      # (y,x,y+h,x+w) gives left bound, upper bound, right bound, bottom\n",
        "      img = img.crop((bbox.origin_x, bbox.origin_y, bbox.origin_x + bbox.width, bbox.origin_y + bbox.height))\n",
        "      #img = np.array(img)[:, :, ::-1]\n",
        "      img = np.array(img)\n",
        "      img = cv2.GaussianBlur(img, (7, 7), 0)\n",
        "      laplacian = cv2.Laplacian(img, cv2.CV_64F)\n",
        "      lapval = np.std(laplacian)\n",
        "      if lapval > laplacemax:\n",
        "        laplacemax = lapval\n",
        "        stepmax = int(str(os.path.basename(StepNo))[0:3])\n",
        "  return stepmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lamt2zctUn4"
      },
      "source": [
        "The instantiation of the 2 models and load of the model wegiths, as detailed in the original MediaPipe tutorial notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAAqX-F3F_0M"
      },
      "outputs": [],
      "source": [
        "from pickle import FALSE\n",
        "# STEP 1: Import the necessary modules.\n",
        "import mediapipe as mp\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "num_blanks = 0\n",
        "# STEP 2: Create an FaceLandmarker object.\n",
        "base_options = python.BaseOptions(model_asset_path='face_landmarker_v2_with_blendshapes.task')\n",
        "options = vision.FaceLandmarkerOptions(base_options=base_options,\n",
        "                                       output_face_blendshapes=False,\n",
        "                                       output_facial_transformation_matrixes=False,\n",
        "                                       num_faces=1)\n",
        "detector = vision.FaceLandmarker.create_from_options(options)\n",
        "#face_detection\n",
        "base_options_face = python.BaseOptions(model_asset_path='detector.tflite')\n",
        "face_options = vision.FaceDetectorOptions(base_options=base_options_face)\n",
        "face_detector = vision.FaceDetector.create_from_options(face_options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bKUlTmkGLwr"
      },
      "source": [
        "Key landmark indices (as defined in MediaPipe Model Documentation [here](https://developers.google.com/mediapipe/solutions/vision/face_landmarker#models)):\n",
        "- 4: tip of nose\n",
        "- 8: base of nose\n",
        "- 9 between eyebrows\n",
        "- 473: right iris\n",
        "- 468: left iris\n",
        "- 0: top of lip (center)\n",
        "- 17 bottom of lip (center)\n",
        "- 57: L mouth\n",
        "- 287: R mouth\n",
        "- 10: center forehead\n",
        "- 332: R forehead\n",
        "- 103: L forehead\n",
        "- 152: bottom of chin\n",
        "- 48: L nose\n",
        "- 278: R nose\n",
        "- 446: R R eye\n",
        "- 464: L R eye\n",
        "- 243: R L eye\n",
        "- 226: L L eye"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTrNCcCdt2sV"
      },
      "source": [
        "The below loop performs the following for each session for each participant:\n",
        "1. Define empty facial landmark dictionary to store the positions of the 478 facial features\n",
        "2. Detect the face in the in-focus image using the MediaPipe face detector\n",
        "3. Crop the image to the bounding box of the face\n",
        "4. Find the step setting of the camera that corresponds to the most in-focus image using laplace_max_step\n",
        "5. Detect the facial landmarks on the most in-focus image using MediaPipe landmarker\n",
        "6. Record all positions, timestamps, and estimate depth to overall face based on calibration of steps to depth (in cm) as calibrated in a separate data collection.\n",
        "7. Loop through the facial landmark values once all detections are complete to create a temporal-based measurement of net distance traveled across x, y, and z axes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eyz2ERzJuuSF"
      },
      "outputs": [],
      "source": [
        "# tk\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "todo = [30]\n",
        "#todo = [23,28,30]\n",
        "done = []\n",
        "\n",
        "for sb in todo:\n",
        "  if sb in done:\n",
        "    continue\n",
        "  sub = f'e{sb}'\n",
        "  print(sub)\n",
        "  path = pth = f'/content/drive/MyDrive/Fatigue Project - Zhang/1-HRC data collection and processing/Data_Collection_Raw/{sub}/1_Facial_Raw/{sub}/{sub}'\n",
        "  end_pth = '/content/drive/MyDrive/Fatigue Project - Zhang/Results-Dec/'\n",
        "  for test in ['baseline', 'session1', 'session2', 'session3', 'session4', 'session5', 'session6', 'session7', 'session8']:\n",
        "    tst_path = f'{path} {test}'\n",
        "    land_dct = {}\n",
        "    face_depth = []\n",
        "    Second = []\n",
        "    Testname = []\n",
        "    sub_name = []\n",
        "    face_wid = 17.465\n",
        "    mvmt = []\n",
        "    for i in range(478):\n",
        "      land_dct[f'feat_{i}_x'] = []\n",
        "      land_dct[f'feat_{i}_y'] = []\n",
        "      land_dct[f'feat_{i}_z'] = []\n",
        "    try:\n",
        "      os.scandir(tst_path)\n",
        "    except FileNotFoundError:\n",
        "      continue\n",
        "    for minute in os.scandir(tst_path):\n",
        "      if minute is None:\n",
        "        continue\n",
        "      for second in os.scandir(minute):\n",
        "        if second is None:\n",
        "          continue\n",
        "        path2 = tst_path + '/' + str(os.path.basename(minute)) + '/' + str(os.path.basename(second)) + '/'\n",
        "        #print path so that we can see what image triggered a particular error\n",
        "        try:\n",
        "          stepmax = laplace_max_step(path2)\n",
        "        except UnboundLocalError:\n",
        "          continue\n",
        "        pathname_max = os.path.join(path2, str(stepmax) + '.jpg')\n",
        "\n",
        "        try:\n",
        "          focused_image = mp.Image.create_from_file(pathname_max)\n",
        "        except RuntimeError:\n",
        "          continue\n",
        "\n",
        "        focused_image2 = cv2.imread(pathname_max, cv2.IMREAD_GRAYSCALE)\n",
        "        if focused_image is None:\n",
        "          continue\n",
        "        detection_result = detector.detect(focused_image)\n",
        "        (x_max,y_max) = focused_image2.shape\n",
        "        try:\n",
        "          rslt = detection_result.face_landmarks[0]\n",
        "        except IndexError:\n",
        "          continue\n",
        "        for i in range(len(rslt)):\n",
        "          land_dct[f'feat_{i}_x'].append(rslt[i].x * x_max)\n",
        "          land_dct[f'feat_{i}_y'].append(rslt[i].y * y_max)\n",
        "          land_dct[f'feat_{i}_z'].append(rslt[i].z * face_wid)\n",
        "\n",
        "        Second.append((int(os.path.basename(minute))* 60) + int(os.path.basename(second)))\n",
        "        Testname.append(str(test))\n",
        "        sub_name.append(sub)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #converts the above step value into depth in centimeters\n",
        "        cm_depth = 2.54 * (282.37 - 0.54 * int(stepmax))\n",
        "\n",
        "        #add to face_depth list\n",
        "        face_depth.append(cm_depth)\n",
        "\n",
        "\n",
        "    df_og = pd.DataFrame({'Subject Name': sub_name, 'Test Name': Testname,'Time (s)':Second, 'Face Depth (cm)':face_depth, **land_dct})\n",
        "    df = df_og.where((df_og['Test Name'] == f'{test}')).dropna().sort_values('Time (s)').set_index('Time (s)')\n",
        "    lst_xdiff = []\n",
        "    lst_ydiff = []\n",
        "    lst_zdiff = []\n",
        "    lst_xdiff_abs = []\n",
        "    lst_ydiff_abs = []\n",
        "    for i in range(478):\n",
        "      df[f'feat_{i}_ztot'] = -1 * df[f'feat_{i}_z'] + df['Face Depth (cm)']\n",
        "      #mag = abs(df[f'feat_{i}_x'].diff()) + abs(df[f'feat_{i}_y'].diff()) + abs(df[f'feat_{i}_ztot'].diff())\n",
        "      mag = abs(df[f'feat_{i}_x'].diff()) + abs(df[f'feat_{i}_y'].diff())\n",
        "      # df[f'feat_{i}_xydiff'] = mag\n",
        "      # df[f'feat_{i}_xydiff'].fillna(0, inplace=True)\n",
        "      df[f'feat_{i}_xdiff'] = df[f'feat_{i}_x'].diff()\n",
        "      df[f'feat_{i}_xdiff'].fillna(0, inplace=True)\n",
        "      df[f'feat_{i}_ydiff'] = df[f'feat_{i}_y'].diff()\n",
        "      df[f'feat_{i}_ydiff'].fillna(0, inplace=True)\n",
        "      df[f'feat_{i}_zdiff'] = abs(df[f'feat_{i}_ztot'].diff())\n",
        "      df[f'feat_{i}_zdiff'].fillna(0, inplace=True)\n",
        "\n",
        "      lst_xdiff.append(np.sum(df[f'feat_{i}_xdiff']))\n",
        "      lst_ydiff.append(np.sum(df[f'feat_{i}_ydiff']))\n",
        "      lst_zdiff.append(np.sum(df[f'feat_{i}_zdiff']))\n",
        "\n",
        "      lst_xdiff_abs.append(np.sum(abs(df[f'feat_{i}_xdiff'])))\n",
        "      lst_ydiff_abs.append(np.sum(abs(df[f'feat_{i}_ydiff'])))\n",
        "\n",
        "    df_sums = pd.DataFrame({'Feature Number': range(478),\n",
        "                      'X Disp': lst_xdiff,\n",
        "                      'Y Disp': lst_ydiff,\n",
        "                      'X Abs': lst_xdiff_abs,\n",
        "                      'Y Abs': lst_ydiff_abs})\n",
        "\n",
        "    csv_nm = f'{sub}-{test}.csv'\n",
        "    csv_sumsnm = f'{sub}-{test}-SUMS.csv'\n",
        "    #os.path.join(f'{end_pth}{sub}_processed/1_Facial_Processed', csv_nm)\n",
        "    df.to_csv(os.path.join(end_pth, csv_nm))\n",
        "    df_sums.to_csv(os.path.join(end_pth, csv_sumsnm))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}